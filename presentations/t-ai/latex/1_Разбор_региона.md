---
title: "Разбор: тестовый регион + регион (1-й тур) ВсОШ по ИИ 2025/2026"
author: "Калашников Александр"
institute: "Wildberries & Russ, ФКН ВШЭ"
format:
  beamer:
    pdf-engine: xelatex
    aspectratio: 169
    fontsize: 9pt
    section-titles: true
    incremental: true
    include-in-header: ./xeheader.tex # Убедитесь, что путь к файлу верный
header-includes:
  - \newcommand{\bgimage}{./logo2.png} # Убедитесь, что путь к картинке верный
---

# 1. Аналитика и структура задач

## Матрица компетенций в ИИ

Анализ 16 задач (8 тестовых + 8 из регионального тура) показывает, что они образуют идеальную матрицу из 4 фундаментальных областей Computer Science и Data Science. Успешное решение требует синтеза строгой математики и понимания метрик машинного обучения.

::: {.nonincremental}
1. **Машинное обучение и Функции потерь (5 задач)**
   * *Тест G (RMSE)* и *Регион D (MAE)*: Свойства метрик $L_1$ и $L_2$.
   * *Тест H (Дрон)* и *Регион H (Одинокий круг)*: Фиттинг, Гаусс-Ньютон, SVM и нелинейные ядра.
   * *Регион E (Прямая крутится)*: Геометрия линейного классификатора.
2. **Теория вероятностей и Случайные процессы (4 задачи)**
   * *Тест A (Незнайка)* и *Тест F (Гладиаторы)*: Инварианты, мартингалы, теорема об остановке.
   * *Тест E (Муравей)* и *Регион F (NLP-строки)*: Цепи Маркова, поглощающие состояния, СЛАУ.
3. **Оптимизация и Математический анализ (3 задачи)**
   * *Тест B (Энтропия)* и *Тест D (Минимум $F$)*: Свойства выпуклости, многомерный экстремум.
   * *Регион C (Среднее и медиана)*: Робастная статистика, экстремальные сдвиги.
4. **Алгебра и Комбинаторика (4 задачи)**
   * *Тест C (Инженеры)* и *Регион B (Матрицы)*: Алгебраические упрощения, линейные операторы.
   * *Регион A (Числа)* и *Регион G (Отрезки)*: Рекурренты, числа Фибоначчи.
:::

---

# 2. Теория вероятностей и Марковские процессы

## Инварианты матожидания: Игра Незнайки (Тест A)

**Условие:** Незнайка $n=2025$ раз подкидывает монетку (старт $p=0.5$). При орле $p$ падает на $0.1$ и он получает 1 тугрик. При решке $p$ растёт на $0.1$, выигрыш 0. Сколько тугриков в среднем он выиграет?

**Идея (Матожидание состояний):**
Обозначим состояние $X_k = 10 p_k$. Изначально $X_1 = 5$.
Вычислим матожидание состояния на следующем шаге:
$$
\mathbb{E}[X_{k+1} \mid X_k = x] = (x-1)\frac{x}{10} + (x+1)\left(1 - \frac{x}{10}\right) = 0.8x + 1.
$$
Перейдем к безусловному матожиданию: $m_{k+1} = 0.8m_k + 1$. 
Поскольку $m_1 = 5$, проверяем: $0.8 \cdot 5 + 1 = 5$. Значит, среднее значение $\mathbb{E}[X_k]$ **всегда равно 5**. Ожидаемая вероятность орла на каждом шаге остается $\mathbb{E}[p_k] = 0.5$.

:::{.callout-note}
### Ответ
Средний выигрыш: $0.5 \cdot 2025 = 1012.5$.
:::

## Мартингалы: Турнир гладиаторов (Тест F)

**Условие:** Гладиаторы Тимофея (сумма сил $S_T = 55$) и Максима ($S_M = 30$) сражаются. Боец $x$ побеждает $y$ с вероятностью $\frac{x}{x+y}$ и забирает его силу. Найти вероятность победы Максима в турнире.

**Идея (Мартингал):**
Ожидаемая суммарная сила Тимофея после одного любого боя:
$$
\mathbb{E}[S_T'] = S_T + y \cdot \left(\frac{x}{x+y}\right) - x \cdot \left(\frac{y}{x+y}\right) = S_T.
$$
Мы видим, что математическое ожидание силы не меняется! Процесс является **мартингалом**.

**Теорема об опциональной остановке:**
Турнир кончается, когда $S_T = 85$ (победа Тимофея) или $S_T = 0$ (победа Максима).
$$ \mathbb{E}[S_T^{\text{start}}] = \mathbb{E}[S_T^{\text{end}}] \implies 55 = 85 \cdot P(\text{Тимофей}) + 0 \cdot P(\text{Максим}). $$

:::{.callout-note}
### Ответ
$P(\text{Тимофей}) = \frac{55}{85} \implies P(\text{Максим}) = \frac{30}{85} = \frac{6}{17} \approx 0.352941$.
:::

## Цепи Маркова: Муравей (Тест E) и NLP (Регион F)

**Тест E. Гипер-муравей (Блуждание по кубу)**
Случайное блуждание по 4D-гиперкубу от $0000$ до $1111$. Состояние — расстояние Хэмминга $k$ до цели. Записываем СЛАУ для ожидаемого времени поглощения $E_k$:
$$ E_k = 1 + \frac{k}{4}E_{k-1} + \frac{4-k}{4}E_{k+1} $$
Решение системы дает $E_4 = 64/3 \approx 21.333333$.

**Регион F. NLP (Генерация строк)**
Генерация букв по вероятностям, зависящим от последней буквы. Ищем ожидаемое время до появления `ML`. Пусть $E_X$ — матожидание шагов. Узел `M` особый (если выпадет `L`, игра окончена за 0 дополнительных шагов):
$$ E_M = 1 + \frac{1}{2}(0) + \frac{1}{2}E_N = 1 + 0.5 E_N. $$
Составляем СЛАУ $9 \times 9$, решаем матрично $\implies E_S = 173/12 \approx 14.416667$.

---

# 3. Машинное обучение и Функции потерь

## Свойства метрик: MAE и робастная регрессия (Регион D)

**Условие:** Найти прямую $\hat{y} = ax+b$, минимизирующую абсолютную ошибку $\text{MAE} = \frac{1}{N} \sum |ax_i + b - y_i|$ на заданных 14 точках.

**Инсайт (Свойство медианы):**
Сумма модулей отклонений $\sum |c - y_i|$ минимальна тогда и только тогда, когда $c$ является **медианой** набора $y_i$. (В отличие от MSE, который минимизируется средним).

Сгруппируем точки датасета по оси $X$. Для каждого $X$ оптимальное предсказание должно лежать в медианном интервале значений $Y$:
* При $x=0 \implies y \in \{0,2,7,8\}$, медиана $\in [2,7]$ (ошибка = 13)
* При $x=4 \implies y \in \{4,7\}$, медиана $\in [4,7]$ (ошибка = 3)
* И так далее...

Если существует прямая сквозь **все** эти медианные интервалы, она глобально оптимальна. Горизонтальная прямая $y=4$ идеально проходит через все интервалы!

:::{.callout-note}
### Ответ
Итоговая сумма = 34, $\text{MAE} = 34 / 14 = 17/7 \approx 2.428571$.
:::

## SVM и Kernel Trick: Одинокий круг (Регион H)

**Условие:** Разделить точки двух классов окружностью. Разрешено использовать только линейный классификатор `SVC(kernel='linear')`.

**Feature Mapping (Скрытая линейность):**
Уравнение окружности $(x-x_0)^2 + (y-y_0)^2 = R^2$ в координатах $(x, y)$ нелинейно.
Но давайте раскроем скобки:
$$ 1 \cdot (x^2 + y^2) - 2x_0 \cdot x - 2y_0 \cdot y + (x_0^2 + y_0^2 - R^2) = 0 $$
Если мы добавим новый признак $z_1 = x^2+y^2$, уравнение становится **идеально линейным** в 3D пространстве $(x^2+y^2, x, y)$!
$$ w_0 \cdot (x^2+y^2) + w_1 \cdot x + w_2 \cdot y + b = 0 $$

**Алгоритм:**
1. Генерируем новый признак: $x^2+y^2$.
2. Нормализуем фичи (`StandardScaler`), иначе квадраты сломают градиенты. Обучаем `SVC`.
3. Восстанавливаем геометрию: $x_0 = -\frac{w_1}{2w_0}, \quad y_0 = -\frac{w_2}{2w_0}, \quad R = \sqrt{x_0^2+y_0^2 - \frac{b}{w_0}}$.

## Геометрия и Симметрия: Прямая крутится (Регион E)

**Условие:** Оценить максимальную долю площади прямоугольника, которую правильно предскажет линейный классификатор, проходящий через его центр.

**Инсайт (Центральная симметрия):**
Разобьем все пиксели на пары, симметричные относительно центра поля.
* **Одноцветные пары (0-0 или 1-1):** Любая прямая, проходящая через центр, всегда классифицирует ровно один пиксель верно, а другой — ложно. Их вклад в точность строго **$50\%$**, независимо от угла прямой!
* **Разноцветные пары (0-1):** Всегда существует угол наклона, который предскажет оба пикселя со **$100\%$** точностью.

**Вывод:** Линейный классификатор бессилен перед центральной симметрией одного класса. На итоговую точность влияет только подбор правильного угла для разноцветных пар.

---

# 4. Оптимизация и Математический анализ

## Выпуклость и Энтропия (Тест B)

**Условие:** Найти максимум $H(p) = 1 - \sum p_i^2$, если $\sum p_i = 1$, все $p_i \ge 0.01$, и $\exists p_j = 0.2$.

**Идея (Свойства выпуклости):**
Максимизация $H(p)$ эквивалентна минимизации суммы квадратов $\sum p_i^2$.
Функция $f(x) = x^2$ строго выпукла. Значит, при фиксированной сумме её значение минимизируется, когда слагаемые «размазаны» (максимально равны друг другу).
Один исход забирает $0.2$. Остаётся масса $0.8$.
Чтобы сделать вероятности максимально равными при ограничении снизу $\ge 0.01$, нужно распределить $0.8$ на максимальное число исходов $m \le 80$. Берем $80$ исходов ровно по $0.01$.

:::{.callout-note}
### Ответ
$H_{\max} = 1 - (0.2^2 + 80 \cdot 0.01^2) = 1 - 0.048 = 0.952$.
:::

## Среднее и медиана (Регион C)

**Условие:** 10 точек на отрезке $[0, 1]$. Зазоры $|x_i - x_j| \ge 0.01$. Любой подотрезок длины $0.25$ содержит точку. Найти максимальную разницу $|\bar{x} - m|$.

**Идея (Экстремальные сдвиги):**
Чтобы максимизировать $(\bar{x} - m)$, нужно "утащить" среднее вправо (к 1), а медиану $m = \frac{x_5 + x_6}{2}$ влево (к 0).
* Левую половину прижимаем к $0$ с минимальным шагом $0.01$: $\{0, 0.01, 0.02, 0.03, 0.04\}$.
* Чтобы минимизировать $m$, прижимаем $x_6$ вплотную к $x_5$: $0.05$. (Итого $m = 0.045$).
* Правую половину раскидываем вправо, закрывая пустоты с максимальным шагом $0.25$: $\{0.30, 0.55, 0.80, 1.00\}$.

Для этой расстановки среднее $\bar{x} = 0.28$. Разница: $0.28 - 0.045 = 0.235$.

## Минимизация функции 3 переменных (Тест D)

**Условие:** Найти точку минимума $F(x,y,z) = y^4 + 2y^2 + z^4 + 4z\cos x + 4yz\sin x$ при $y,z \ge 0$.

**Решение по шагам:**
1. **Свёртка по $x$:** Тригонометрия $4z(\cos x + y\sin x) = 4z\sqrt{1+y^2}\cos(x-\varphi)$. Минимум по $x$ равен $-4z\sqrt{1+y^2}$ (косинус $=-1$).
2. **Оптимизация по $z$:** Пусть $r = \sqrt{1+y^2}$. Минимизируем $z^4 - 4rz$. Производная даёт $z = r^{1/3}$. Подставляем, получаем минимум $-3r^{4/3}$.
3. **Оптимизация по $y$:** Минимизируем $f(y) = y^4 + 2y^2 - 3(1+y^2)^{2/3}$. Делаем замену $t=y^2 \ge 0$. Анализ производной показывает единственный экстремум при $t=0 \implies y=0$.

:::{.callout-note}
### Ответ
Возвращаясь назад: $r=1, z=1, x=\pi$. Сумма $x+y+z \approx 4.141593$.
:::

---

# 5. Линейная алгебра и Комбинаторика

## Матрицы и забытые активации (Регион B)

**Условие:** Нейросеть без нелинейных активаций и bias делает преобразование $\mathbb{R}^2 \to \mathbb{R}^3$. Затем применяется покомпонентный `clip` в $[0, 255]$.
Дано: $f(-2,3) = (60, 40, 100)^T$, $f(1,3) = (200, 50, 80)^T$. Найти $f(-7,6)$.

**Инсайт (Линейность):**
Сеть, состоящая исключительно из последовательных умножений матриц ($W_L \dots W_2 W_1 x = A x$), представляет собой классическое **линейное преобразование** $T(x)$.
Выразим искомый вектор через известный базис: $(-7, 6) = 3 \cdot (-2, 3) - 1 \cdot (1, 3)$.
Пользуясь свойством линейности $T(\alpha u + \beta v) = \alpha T(u) + \beta T(v)$:
$$
T(-7, 6) = 3 \begin{pmatrix}60\\40\\100\end{pmatrix} - \begin{pmatrix}200\\50\\80\end{pmatrix} = \begin{pmatrix}-20\\70\\220\end{pmatrix}.
$$
После обрезки `clip` отрицательное число зануляется. Ответ: **0 70 220**.

## Комбинаторные последовательности: Фибоначчи (Регион G)

**Условие:** Разрешено пересечение не более 2 временных отрезков. Найти число способов выбрать непересекающийся набор.

**Динамическое программирование:**
Пусть $f(n)$ — количество способов выбрать непересекающиеся отрезки из пересекающейся цепочки.
* Если **берём** $n$-й отрезок, то $(n-1)$-й брать нельзя. Остаётся $f(n-2)$ вариантов.
* Если **не берём** $n$-й отрезок, остаётся $f(n-1)$ вариантов.
Рекуррента $f(n) = f(n-1) + f(n-2)$ порождает **числа Фибоначчи**. 
Для независимых графов-цепочек общее число выборов — произведение чисел Фибоначчи $\prod F_i$.

## Префиксные суммы (Регион A)

**Условие:** Последовательность $a_n = \sum_{i=1}^{n-1} a_i$. Одно из чисел $N=123456123456123456$. Найти $a_1$.

**Свойство:**
Пусть $S_n = \sum_{i=1}^n a_i$. Тогда $S_n = S_{n-1} + a_n = 2S_{n-1}$.
Суммы растут как степени двойки: $S_n = 2^{n-2}(a_1 + 1)$.
Само число $a_n = S_{n-1} = 2^{n-3}(a_1 + 1)$.
Значит $N = 2^k(a_1+1)$, откуда $a_1 = \frac{N}{2^k} - 1$. Ответами будут все значения для $k$, при которых $N$ делится на $2^k$ нацело.

---

# 6. Приложения: Эвристики и Код

## Быстрый пересчет OLS за O(1) (Тест G - RMSE)

Для локального поиска (удаление/добавление точек) переобучать линейную регрессию за $O(N)$ слишком долго (Time Limit).
Коэффициенты линейной регрессии зависят лишь от 5 сумм. Если поддерживать эти суммы, добавление/удаление точки работает за $O(1)$.

\footnotesize
```python
def compute_sse(n, sx, sy, sxx, sxy, syy):
    if n <= 2: return 0.0
    varx = sxx - (sx * sx) / n
    cov  = sxy - (sx * sy) / n
    
    a = cov / varx if abs(varx) > 1e-18 else 0.0
    b = (sy / n) - a * (sx / n)
    
    # Итоговая ошибка пересчитывается мгновенно:
    sse = syy + a*a*sxx + n*b*b - 2*a*sxy - 2*b*sy + 2*a*b*sx
    return max(0.0, sse)

```

\normalsize

## Метод Монте-Карло как спасение (Регион F - NLP)

Если на вывод математической СЛАУ уходит слишком много времени, вероятностные задачи идеально решаются симуляцией (Закон Больших Чисел).

\footnotesize

```python
import random
next_letters = {"A": ["M", "D"], "T": ["L", "N"], "M": ["L", "N"], 
                "E": ["T", "S", "M"], "I": ["T", "S", "M"], ...}

def simulate():
    last = "S"; steps = 0
    while True:
        nxt = random.choice(next_letters[last])
        steps += 1
        if last == "M" and nxt == "L": return steps
        last = nxt

# Среднее по 10 млн запусков гарантированно сходится к 14.4166...
ans = sum(simulate() for _ in range(10_000_000)) / 10_000_000

```

\normalsize

---

# Итоги и Выводы

## Главные инсайты (Что нужно знать для олимпиад по ИИ)

::: {.nonincremental}

1. **Скрытая линейность (Kernel Trick):** Нейросети без активаций вырождаются в одну матрицу. Сложные нелинейные кривые (круги, эллипсы) превращаются в плоскости при добавлении полиномиальных фичей. Ищите линейность!
2. **Свойства Loss-функций:**  ошибка (MAE) минимизируется медианой,  ошибка (MSE/RMSE) — средним арифметическим. Использование этого факта снижает алгоритмическую сложность решения задач.
3. **Инварианты в случайных процессах:** В задачах с играми всегда ищите мартингалы — величины, чьё матожидание не меняется. Теорема об опциональной остановке даёт ответ мгновенно, избавляя от бесконечных рядов.
4. **Умные эвристики:** Когда аналитическое решение громоздко, алгоритмы локального поиска (Random Swap) с пересчетом за  или симуляции Монте-Карло позволяют забрать полный балл.
:::