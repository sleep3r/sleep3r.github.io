---
title: "Линейная алгебра"
author: "Калашников Александр"
institute: "Wildberries & Russ, ФКН ВШЭ"
format:
  beamer:
    pdf-engine: xelatex
    aspectratio: 169
    fontsize: 10pt
    section-titles: true
    incremental: false
    include-in-header: ./xeheader.tex
header-includes:
  - \newcommand{\bgimage}{./logo2.png}
  - \usepackage{tikz}
  - \usetikzlibrary{shapes.geometric, arrows.meta, positioning, calc, decorations.pathreplacing}
  - \usepackage{amsmath, amssymb, bm}
---

# 1. Теоретический арсенал

## Почему матрицы в ML — это не просто таблички?

В школе и на первых курсах много времени уходит на метод Гаусса и скучное перемножение индексов. Но в машинном обучении за каждой матрицей кроется **геометрическое преобразование**, **граф связей** или **оператор оптимизации**.

:::: {.columns}
::: {.column width="55%"}
**План на теоретический блок:**

1. Скрытые свойства функций активации (Sigmoid, Softmax).
2. Матричный вид градиентного спуска.
3. Инварианты: След, Спектр и Определитель.
4. Матрицы Грама и некоммутативная алгебра.
5. Теорема Кэли-Гамильтона.
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[scale=0.8, thick, >=Stealth]
  \node[circle, draw=blue, fill=blue!10, minimum size=1cm] (A) at (0, 2) {$W$};
  \node[circle, draw=red, fill=red!10, minimum size=1cm] (x) at (2, 0) {$x$};
  \node[circle, draw=green!60!black, fill=green!10, minimum size=1cm] (Ax) at (-2, 0) {$Wx$};
  \draw[->, line width=1.5pt] (x) -- node[above right] {Умножение} (A);
  \draw[->, line width=1.5pt] (A) -- node[above left] {Преобразование} (Ax);
\end{tikzpicture}

\vspace{0.2cm}
{\small Матрица — это оператор искажения пространства!}
\end{center}
:::
::::

## Концепт 1: Математика ML-активаций

Без знания алгебраических свойств активаций анализировать нейросети невозможно.

:::: {.columns}
::: {.column width="50%"}
**Симметрия Сигмоиды ($\sigma$)**
Функция $\sigma(z) = \frac{1}{1 + e^{-z}}$ обладает центральной симметрией относительно точки $(0, 0.5)$:
$$ \sigma(-z) = 1 - \sigma(z) $$
Это позволяет сводить сложные уравнения вида $\sigma(x) = 1 - \sigma(y)$ к линейным $x = -y$.

**Свойства Softmax**
Softmax превращает вектор сырых оценок $z$ в распределение вероятностей $p$:
$$ p_i = \frac{e^{z_i}}{\sum e^{z_j}} $$
Главный инвариант: сумма всех $p_i$ **всегда равна $1$**.
:::
::: {.column width="50%"}
\begin{center}
\begin{tikzpicture}[scale=1, thick]
  \draw[->] (-3,0) -- (3,0) node[right] {$z$};
  \draw[->] (0,-0.2) -- (0,1.5) node[above] {$\sigma(z)$};
  \draw[dashed] (-3,1) -- (3,1) node[right] {$1$};
  
  \draw[domain=-3:3, smooth, variable=\x, blue, very thick] plot ({\x}, {1 / (1 + exp(-\x))});
  
  \filldraw[red] (1.5, 0.817) circle (2pt) node[above left] {$\sigma(z)$};
  \filldraw[red] (-1.5, 0.182) circle (2pt) node[below right] {$\sigma(-z)$};
  \draw[dashed, red] (-1.5, 0.182) -- (1.5, 0.817);
  \filldraw[black] (0, 0.5) circle (2pt) node[below right] {$0.5$};
\end{tikzpicture}

\vspace{0.2cm}
{\small Центральная симметрия $\sigma(-z) = 1 - \sigma(z)$}
\end{center}
:::
::::

## Концепт 2: Градиентный спуск в матрицах

В ML мы минимизируем функцию потерь $f(x)$, шагая против градиента.

:::: {.columns}
::: {.column width="55%"}
**Правило шага (GD):**
$$ x_{k+1} = x_k - \eta \cdot \nabla f(x_k) $$
Где $\eta$ (learning rate) — размер шага.

Если функция квадратичная $f(x) = x^T H x$, то её градиент линейно зависит от $x$: $\nabla f(x) = Hx$.
Тогда шаг GD превращается в **умножение на матрицу**:
$$ x_{k+1} = x_k - \eta H x_k = \mathbf{(I - \eta H) x_k} $$
Поведение всего алгоритма (сойдется, взорвется или зациклится) зависит от **спектра матрицы** $(I - \eta H)$.
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[scale=0.7, thick, >=Stealth]
  \draw[->, gray] (-2.5, 0) -- (2.5, 0);
  \draw[->, gray] (0, -1) -- (0, 3.5);
  
  \draw[domain=-1.8:1.8, smooth, variable=\x, blue] plot ({\x}, {\x*\x});
  \filldraw[red] (1.5, 2.25) circle (2pt) node[right] {$x_k$};
  \filldraw[red] (-1.0, 1.0) circle (2pt) node[left] {$x_{k+1}$};
  \draw[->, dashed, orange, line width=1pt] (1.5, 2.25) -- (-1.0, 1.0) node[midway, above] {$\times (I - \eta H)$};
\end{tikzpicture}

\vspace{0.2cm}
{\small Если процесс прыгает туда-сюда, матрица имеет $\lambda = -1$}
\end{center}
:::
::::

## Концепт 3: Инварианты (След, Спектр, Определитель)

Любая квадратная матрица $A$ имеет набор собственных чисел (спектр) $\lambda_1, \dots, \lambda_n$.

:::: {.columns}
::: {.column width="55%"}
**Главные тождества:**

1. **След (сумма диагонали):**
   $\operatorname{tr}(A) = \sum \lambda_i$
2. **Определитель (объём):**
   $\det(A) = \prod \lambda_i$
3. **Степени матрицы:**
   Собственные числа $A^p$ — это $\lambda_i^p$.
   $\operatorname{tr}(A^p) = \sum \lambda_i^p$

\vspace{0.3cm}
**Графовый смысл следа:**
Если $A$ — матрица смежности графа, то элемент $(A^p)_{ii}$ — это число путей из узла $i$ в себя за $p$ шагов. След $\operatorname{tr}(A^p)$ — это общее количество замкнутых циклов длины $p$.
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[thick, >=Stealth]
  \node[circle, draw, fill=blue!10] (1) at (0, 1.5) {1};
  \node[circle, draw, fill=blue!10] (2) at (2, 1.5) {2};
  \node[circle, draw, fill=blue!10] (3) at (1, 0) {3};
  \draw[->] (1) edge[bend left] (2);
  \draw[->] (2) edge[bend left] (3);
  \draw[->] (3) edge[bend left] (1);
  \draw[->] (1) edge[loop left] (1);
\end{tikzpicture}

\vspace{0.2cm}
{\small Возведение графа в степень $p$ позволяет считать пути}
\end{center}
:::
::::

## Концепт 4: Матрицы Грама и Некоммутативная алгебра

:::: {.columns}
::: {.column width="50%"}
**Матрица Грама ($A A^T$)**
Произведение матрицы $A$ на саму себя транспонированную состоит из попарных скалярных произведений строк:
$(AA^T)_{ij} = \langle \text{строка } i, \text{строка } j \rangle$

Если строки ортогональны и имеют одинаковую длину $c$, то все внедиагональные элементы зануляются:
$$ \mathbf{A A^T = c \cdot I} $$
:::
::: {.column width="50%"}
**Вынесение за скобки**
В матрицах нет дробей, делить нельзя. Но можно ловко выносить инверсии:
$$ A^{-1} + B^{-1} = A^{-1}A B^{-1} + A^{-1} B B^{-1} $$
$$ = A^{-1}(B + A)B^{-1} $$

**Мощное свойство обратимости:**
Для *квадратных* матриц: если $AB = I$, то гарантированно $BA = I$.
:::
::::

\begin{center}
\vspace{0.3cm}
{\small Это свойство — чит-код для олимпиадных задач}
\end{center}

## Концепт 5: Теорема Кэли-Гамильтона

Вспомним характеристический многочлен матрицы: $\chi_A(t) = \det(tI - A)$. Его корни — собственные значения.

::: {.block title="Главная теорема"}
Всякая квадратная матрица является корнем своего характеристического многочлена.
То есть $\chi_A(A) = \mathbf{0}$.
:::

:::: {.columns}
::: {.column width="60%"}
**Зачем это нужно?**
Для матрицы $3 \times 3$ многочлен выглядит так (по теореме Виета):
$$ t^3 - \operatorname{tr}(A)t^2 + \dots - \det(A) = 0 $$
Подставляем матрицу $A$:
$$ A^3 - \operatorname{tr}(A)A^2 + \dots - \det(A)I = \mathbf{0} $$

Это позволяет мгновенно понижать гигантские степени матрицы!
:::
::: {.column width="40%"}
\begin{center}
\begin{tikzpicture}
  \node[draw, fill=yellow!10, rounded corners, text width=3.5cm, align=center, inner sep=5pt] {
    Матрица удовлетворяет своему же уравнению!\\
    (Число заменяется на единичную матрицу $I$).
  };
\end{tikzpicture}
\end{center}
:::
::::

---

# 2. Задача 1: Неподвижная точка Softmax (Разминка)

## Условие задачи

Лёша обучил логистическую регрессию без свободного члена для 2 классов: $p(x) = \mathrm{softmax}(Wx)$.
Матрица весов $W = \begin{pmatrix} 7 & 5 \\ 8 & 6 \end{pmatrix}$.
Для некоторой точки $x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$ оказалось, что $p(x) = x$.

**Вопрос:** Найдите значение $x_1$.

*(Сложность: Легкая. Проверяем понимание ML-выходов).*

## Решение: Свойства вероятностей

Поскольку выход модели $p(x)$ — это распределение вероятностей (после Softmax), сумма его компонент равна $1$, и они положительны. Так как $p(x) = x$, то **$x_1 + x_2 = 1$**.

:::: {.columns}
::: {.column width="55%"}
Посчитаем логиты $z = Wx$:
$$ z_1 = 7x_1 + 5x_2 $$
$$ z_2 = 8x_1 + 6x_2 $$

Распишем $p_1$ по формуле Softmax:
$$ p_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2 - z_1}} $$
:::
::: {.column width="45%"}
Найдём разность логитов:
$$ z_2 - z_1 = (8x_1 + 6x_2) - (7x_1 + 5x_2) $$
$$ = x_1 + x_2 $$

А мы уже знаем, что $x_1 + x_2 = 1$!
Значит, $z_2 - z_1 = \mathbf{1}$.

\vspace{0.3cm}
\begin{block}{Ответ}
$$ x_1 = p_1 = \mathbf{\frac{1}{1 + e}} $$
\end{block}
:::
::::

---

# 3. Задача 2: Секрет Полносвязного Слоя

## Условие задачи

Нейросеть задана формулой: $NN(X) = \mathrm{Sigmoid}( (X W_1 + b_1) W_2)$.
Размерности: $X \in \mathbb{R}^{1 \times 2}$, $W_1 \in \mathbb{R}^{2 \times 2}$, $b_1 \in \mathbb{R}^{1 \times 2}$, $W_2 \in \mathbb{R}^{2 \times 1}$.
Известно: $b_1 = (-2, 2)$, $W_2 = (1, 1)^T$.
Оказалось, что: $NN(2, 2) = 2 \cdot NN(-2, -2)$.

**Вопрос:** Найдите сумму всех элементов матрицы $W_1$.

*(Сложность: Легкая).*

## Решение: Свертка в скаляр

Обозначим сумму элементов $W_1$ как $S$.
Умножение вектора на столбец $W_2 = (1,1)^T$ — это просто **сумма элементов вектора**.
Для $X=(2,2)$, вектор $X W_1$ содержит элементы $2w_{11}+2w_{21}$ и $2w_{12}+2w_{22}$. Их сумма равна $2S$.
Сумма элементов вектора $b_1$ равна $-2 + 2 = \mathbf{0}$.

:::: {.columns}
::: {.column width="50%"}
Значит, вход в сигмоиду для $X=(2,2)$ равен $2S$. А для $X=(-2,-2)$ вход равен $-2S$.

Условие задачи принимает вид:
$$ \sigma(2S) = 2 \sigma(-2S) $$

Свойство сигмоиды из теории: $\sigma(-z) = 1 - \sigma(z)$.
$$ \sigma(2S) = 2 (1 - \sigma(2S)) $$
$$ 3 \sigma(2S) = 2 \implies \sigma(2S) = \frac{2}{3} $$
:::
::: {.column width="50%"}
Раскрываем сигмоиду:
$$ \frac{1}{1 + e^{-2S}} = \frac{2}{3} $$
$$ 1 + e^{-2S} = \frac{3}{2} \implies e^{-2S} = \frac{1}{2} $$

Логарифмируем:
$$ e^{2S} = 2 \implies 2S = \ln 2 $$

\begin{block}{Ответ}
$$ S = \mathbf{\frac{\ln 2}{2}} $$
\end{block}
:::
::::

---

# 4. Задача 3: Обратный ход Нейросети

## Условие задачи

- **Нейросеть Пети:** $f_1(X) = W_2 \mathrm{ReLU}(W_1 X + b_1) + b_2$
- **Нейросеть Васи:** $f_2(X') = W_4 \mathrm{LeakyReLU}(W_3 X' + b_3) + b_4$ (где $\alpha = 0.01$)

Матрицы весов даны в условии. Петя подаёт $X = (2, 1)^T$.
**Вопрос:** Найдите $X'$, при котором выходы совпадут: $f_1(X) = f_2(X')$.

*(Сложность: Легкая. Тренировка матричного умножения).*

## Прямой ход (Петя)

Считаем по шагам $W_1 = \begin{pmatrix} 1 & 0 \\ 2 & -1 \end{pmatrix}$, $b_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.
$$ W_1 X + b_1 = \begin{pmatrix} 1 & 0 \\ 2 & -1 \end{pmatrix} \begin{pmatrix} 2 \\ 1 \end{pmatrix} + \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 3 \end{pmatrix} + \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \end{pmatrix} $$
Так как числа положительные, $\mathrm{ReLU}$ ничего не меняет: $(3, 4)^T$.

Теперь второй слой $W_2 = \begin{pmatrix} -1 & 0 \\ 1 & -3 \end{pmatrix}$, $b_2 = \begin{pmatrix} 0 \\ -1 \end{pmatrix}$.
$$ Y = \begin{pmatrix} -1 & 0 \\ 1 & -3 \end{pmatrix} \begin{pmatrix} 3 \\ 4 \end{pmatrix} + \begin{pmatrix} 0 \\ -1 \end{pmatrix} = \begin{pmatrix} -3 \\ -9 \end{pmatrix} + \begin{pmatrix} 0 \\ -1 \end{pmatrix} = \begin{pmatrix} \mathbf{-3} \\ \mathbf{-10} \end{pmatrix} $$
Это наша цель для нейросети Васи!

## Обратный ход (Вася)

Решаем уравнение: $W_4 L + b_4 = \begin{pmatrix} -3 \\ -10 \end{pmatrix}$. Учитывая $b_4 = (-1, -1)^T$:
$$ \begin{pmatrix} -1/3 & -1 \\ 1/3 & -1 \end{pmatrix} \begin{pmatrix} l_1 \\ l_2 \end{pmatrix} = \begin{pmatrix} -2 \\ -9 \end{pmatrix} $$
Складывая уравнения, получаем $-2l_2 = -11 \implies l_2 = 5.5$.
Из первого уравнения: $-1/3 l_1 = -2 + 5.5 = 3.5 \implies l_1 = -10.5$.

:::: {.columns}
::: {.column width="50%"}
**Инверсия LeakyReLU:**
Найдём вектор $Z$ до активации.
$l_1 = -10.5 < 0 \implies Z_1 = -10.5 / 0.01 = -1050$.
$l_2 = 5.5 > 0 \implies Z_2 = 5.5$.
:::
::: {.column width="50%"}
**Последний шаг:** $W_3 X' + b_3 = Z$
$$ \begin{pmatrix} 1/2 & 0 \\ 1/2 & -1 \end{pmatrix} X' = \begin{pmatrix} -1050 \\ 5.5 \end{pmatrix} - \begin{pmatrix} -50 \\ 0.5 \end{pmatrix} = \begin{pmatrix} -1000 \\ 5 \end{pmatrix} $$

Из первой строки: $\frac{1}{2} x'_1 = -1000 \implies x'_1 = -2000$.
Из второй: $\frac{1}{2} x'_1 - x'_2 = 5 \implies x'_2 = -1005$.

\begin{block}{Ответ}
$X' = \mathbf{(-2000, -1005)^T}$.
\end{block}
:::
::::

---

# 5. Задача 4: Логистическая граница (Геометрия)

## Условие задачи

Классификатор $f(x) = \sigma(w^T x + b)$ разделяет классы \{1, 2\} от \{3, 4\}.
Точки разбились на пары $(x_1, x_2)$ так, что в паре всегда разные классы **одной чётности** (т.е. \{1 и 3\} или \{2 и 4\}).
Для каждой пары выполнено: $f(x_1) = p$, а $f(x_2) = 1 - p$.

Датасет оказался разбит на 2 куска:

- $A$: классы 1 и 3 $\to A = \{(4, 0), (6, 0), (6, 4), (7, 3), (7, 4), (8, 2), (9, 5), (13, 2)\}$ (8 точек)
- $B$: классы 2 и 4 $\to B = \{(6, 1), (9, 4), (10, 2), (10, 4), (11, 5), (14, 2)\}$ (6 точек)

**Вопрос:** Восстановите прямую $w^T x + b = 0$ и найдите её пересечение с осью Ox. *(Сложность: Средняя).*

## Симметрия Сигмоиды

Вспомним базовое свойство: $1 - \sigma(z) = \sigma(-z)$.
По условию $\sigma(w^T x_1 + b) = 1 - \sigma(w^T x_2 + b) = \sigma(-w^T x_2 - b)$.

:::: {.columns}
::: {.column width="55%"}
Приравниваем логиты:
$$ w^T x_1 + b = -w^T x_2 - b $$
$$ w^T(x_1 + x_2) + 2b = 0 $$
Поделим на 2:
$$ w^T \left( \frac{x_1 + x_2}{2} \right) + b = 0 $$

**Геометрический смысл:** Середина отрезка между точками любой пары лежит точно на разделяющей прямой классификатора!
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[scale=0.6, thick]
  \draw[->, gray] (-1,0) -- (6,0);
  \draw[->, gray] (0,-1) -- (0,5);
  \filldraw[blue] (1, 4) circle (3pt) node[above] {$x_1$ (класс 1)};
  \filldraw[orange] (5, 2) circle (3pt) node[above] {$x_2$ (класс 3)};
  \draw[dashed, gray] (1,4) -- (5,2);
  \filldraw[red] (3, 3) circle (4pt) node[above right] {Середина};
  \draw[thick, green!60!black] (0, 1.5) -- (6, 4.5);
\end{tikzpicture}

\vspace{0.2cm}
{\small Линейность формы позволяет усреднять!}
\end{center}
:::
::::

## Центры масс множеств

Поскольку множество $A$ содержит ТОЛЬКО классы 1 и 3, все пары этих классов целиком лежат внутри $A$.
Если середины всех пар лежат на прямой, то и **общий центр масс** (среднее всех точек) множества $A$ лежит на этой прямой! Аналогично для $B$.

:::: {.columns}
::: {.column width="50%"}
**Центр масс A** (усредняем 8 координат):
$X_A = 60/8 = 7.5$
$Y_A = 20/8 = 2.5$

**Центр масс B** (усредняем 6 координат):
$X_B = 60/6 = 10$
$Y_B = 18/6 = 3$
:::
::: {.column width="50%"}
Строим прямую через $(7.5, 2.5)$ и $(10, 3)$:
Угловой коэффициент $k = \frac{3 - 2.5}{10 - 7.5} = \frac{0.5}{2.5} = 0.2 = \frac{1}{5}$.

Уравнение прямой:
$$ y - 3 = \frac{1}{5}(x - 10) \implies y = \frac{1}{5}x + 1 $$

Пересечение с Ox (где $y=0$):
$$ 0 = \frac{1}{5}x + 1 \implies x = -5 $$

\begin{block}{Ответ}
Координата $x = \mathbf{-5}$.
\end{block}
:::
::::

---

# 6. Задача 5: Оптимизация на GPU (Ковариации)

## Условие задачи

Вы реализуете алгоритм оптимизации второго порядка и получили тяжелую формулу обновления весов, зависящую от двух ковариационных матриц $A$ и $B$:
$$ H_{new} = A - \bigl(A^{-1} + (B^{-1} - A)^{-1}\bigr)^{-1} $$

Каждое взятие обратной матрицы — это очень долго.
**Докажите**, что эту громоздкую формулу можно упростить до красивой и быстрой:
$$ H_{new} = ABA $$

*(Сложность: Средняя).*

## Вынесение матриц за скобки

Обозначим самую проблемную часть $C = B^{-1} - A$. Нам нужно упростить выражение $(A^{-1} + C^{-1})^{-1}$.

:::: {.columns}
::: {.column width="55%"}
В алгебре чисел $x^{-1} + y^{-1} = \frac{x+y}{xy}$. В матрицах дробей нет, но мы можем аккуратно выносить за скобки (как мы смотрели в теории).

Вынесем $A^{-1}$ слева, а $C^{-1}$ справа:
$$ A^{-1} + C^{-1} = A^{-1}A C^{-1} + A^{-1} C C^{-1} $$
$$ = A^{-1} (C + A) C^{-1} $$

Теперь применим операцию обращения ко всему этому произведению, помня, что $(XYZ)^{-1} = Z^{-1} Y^{-1} X^{-1}$:
$$ (A^{-1} + C^{-1})^{-1} = C (C + A)^{-1} A $$
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[thick, >=Stealth]
  \node[draw, rounded corners, fill=red!10, text width=3.5cm, align=center] (heavy) at (0, 1.5) {Тяжёлая версия:\\3 инверсии};
  \node[draw, rounded corners, fill=green!10, text width=3.5cm, align=center] (light) at (0, -1.5) {Лёгкая версия:\\Без инверсий!};
  
  \draw[->, line width=1.5pt] (heavy) -- node[right] {Алгебра} (light);
\end{tikzpicture}
\end{center}
:::
::::

## Финальная подстановка

У нас есть промежуточный результат: $C (C + A)^{-1} A$.
Вспомним, что мы обозначали $C = B^{-1} - A$.

:::: {.columns}
::: {.column width="50%"}
Что такое сумма $(C + A)$?
$$ C + A = (B^{-1} - A) + A = B^{-1} $$

Подставляем это внутрь:
$$ C (B^{-1})^{-1} A = C B A $$
:::
::: {.column width="50%"}
Раскрываем $C$:
$$ (B^{-1} - A) B A = B^{-1} B A - A B A = A - A B A $$

Возвращаемся к изначальной формуле обновления:
$$ H_{new} = A - (A - ABA) = \mathbf{ABA} $$

\begin{block}{Победа}
Это частный случай \textbf{Тождества Вудбери}. Оптимизация ускорена!
\end{block}
:::
::::

---

# 7. Задача 6: Прыгающий Градиент

## Условие задачи

Никита минимизирует функцию $f(x, y) = ax^2 + xy + by^2$.
Он запускает классический градиентный спуск с шагом `learning_rate = 1`.

Оказалось, что алгоритм **никуда не сходится**, а вечно прыгает между двумя **различными** точками: начальной $z_1$ и какой-то $z_2$.
Известно, что $ab \neq 1/4$.

**Вопрос:** Найдите точное численное значение выражения:
$$ (1-a)(1-b) $$

*(Сложность: Сложная. Спектральная теория).*

## Векторизация и шаг спуска

Запишем функцию в матричном виде. Градиент:
$$ \nabla f(z) = \begin{pmatrix} 2ax + y \\ x + 2by \end{pmatrix} = \begin{pmatrix} 2a & 1 \\ 1 & 2b \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = H z $$
Матрица $H$ — это Гессиан. Как мы обсуждали в теории, шаг GD с $\eta = 1$:
$$ z_{k+1} = z_k - 1 \cdot \nabla f(z_k) = z_k - H z_k = \mathbf{(I - H) z_k} $$

:::: {.columns}
::: {.column width="55%"}
Обозначим матрицу перехода $M = I - H$.
По условию, процесс скачет туда-сюда:

- $z_2 = M z_1$
- $z_1 = M z_2$

Подставим одно в другое:
$$ z_1 = M^2 z_1 $$
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[scale=0.8, thick]
  \draw[->, gray] (-2, 0) -- (2, 0);
  \draw[->, gray] (0, -1) -- (0, 2);
  
  \filldraw[blue] (-1.5, 1) circle (2pt) node[left] {$z_1$};
  \filldraw[red] (1.5, 1) circle (2pt) node[right] {$z_2$};
  
  \draw[->, dashed, bend left] (-1.4, 1.1) to node[above, font=\footnotesize] {$M$} (1.4, 1.1);
  \draw[->, dashed, bend left] (1.4, 0.9) to node[below, font=\footnotesize] {$M$} (-1.4, 0.9);
  
  \filldraw[black] (0,0) circle (1pt) node[below right] {Оптимум};
\end{tikzpicture}
\end{center}
:::
::::

## Собственное значение и определитель

Решаем уравнение $(M^2 - I)z_1 = 0 \implies (M-I)(M+I)z_1 = 0$.

- Если бы $(M-I)z_1 = 0$, то $M z_1 = z_1$, и алгоритм стоял бы на месте ($z_2 = z_1$).
- Раз точки *различны*, у $M$ обязано быть собственное значение $\mathbf{\lambda = -1}$.

:::: {.columns}
::: {.column width="50%"}
Следовательно, определитель матрицы $(M - (-I))$ равен нулю:
$$ \det(M + I) = 0 $$

Матрица $M = \begin{pmatrix} 1-2a & -1 \\ -1 & 1-2b \end{pmatrix}$.
$$ M + I = \begin{pmatrix} 2-2a & -1 \\ -1 & 2-2b \end{pmatrix} $$
:::
::: {.column width="50%"}
Считаем определитель:
$$ (2-2a)(2-2b) - (-1)(-1) = 0 $$
$$ 4(1-a)(1-b) - 1 = 0 $$

\vspace{0.5cm}
\begin{block}{Ответ}
$$ (1-a)(1-b) = \mathbf{0.25} $$
\end{block}
:::
::::

---

# 8. Задача 7: 3D-аугментация (Кэли-Гамильтон)

## Условие задачи

Для аугментации данных применяется линейное преобразование $3 \times 3$, заданное матрицей $A$.
Известны 3 факта:

1. Преобразование сохраняет объем: $\det A = 1$.
2. $\operatorname{tr}(A) = 0$.
3. $\operatorname{tr}(A^{-1}) = 0$.

**Докажите**, что если применить эту аугментацию 3 раза подряд, объект вернется в исходное состояние (т.е. $A^3 = I$).

*(Сложность: Сложная).*

## Реконструкция полинома

Пусть $\lambda_1, \lambda_2, \lambda_3$ — собственные значения матрицы $A$.

:::: {.columns}
::: {.column width="55%"}
Запишем условия через $\lambda$:

1. $\lambda_1 \lambda_2 \lambda_3 = 1$
2. $\lambda_1 + \lambda_2 + \lambda_3 = 0$
3. Собственные значения обратной матрицы — это $1/\lambda_i$. Поэтому:
   $\lambda_1^{-1} + \lambda_2^{-1} + \lambda_3^{-1} = 0$

Приведём третье уравнение к общему знаменателю:
$$ \frac{\lambda_2 \lambda_3 + \lambda_1 \lambda_3 + \lambda_1 \lambda_2}{\lambda_1 \lambda_2 \lambda_3} = 0 $$
Так как знаменатель равен $1$, числитель:
**$\lambda_1 \lambda_2 + \lambda_1 \lambda_3 + \lambda_2 \lambda_3 = 0$**.
:::
::: {.column width="45%"}
\begin{center}
\textbf{Характеристический многочлен:}\\
(По теореме Виета)

\vspace{0.3cm}
$\chi_A(t) = t^3 - c_2 t^2 + c_1 t - c_0$
\vspace{0.2cm}

Где:
$c_2 = \sum \lambda_i = 0$ \\
$c_1 = \sum \lambda_i \lambda_j = 0$ \\
$c_0 = \prod \lambda_i = 1$
\end{center}
:::
::::

## Финал через Кэли-Гамильтона

Подставляем найденные коэффициенты в многочлен:
$$ \chi_A(t) = t^3 - 0 \cdot t^2 + 0 \cdot t - 1 = t^3 - 1 $$

:::: {.columns}
::: {.column width="50%"}
Согласно **Теореме Кэли-Гамильтона**, любая матрица обнуляет свой характеристический многочлен:
$$ \chi_A(A) = 0 $$

Подставляем матрицу $A$:
$$ A^3 - I = 0 \implies \mathbf{A^3 = I} $$
:::
::: {.column width="50%"}
\begin{center}
\begin{tikzpicture}[scale=0.9, thick, >=Stealth]
  \node[draw, circle, fill=blue!10, minimum size=1cm] (x) at (0, 0) {$x$};
  \node[draw, circle, fill=orange!10, minimum size=1cm] (ax) at (2, 2) {$Ax$};
  \node[draw, circle, fill=green!10, minimum size=1cm] (aax) at (4, 0) {$A^2x$};

  \draw[->, blue] (x) edge[bend left=20] node[above left] {$A$} (ax);
  \draw[->, orange] (ax) edge[bend left=20] node[above right] {$A$} (aax);
  \draw[->, green!60!black] (aax) edge[bend left=30] node[below] {$A$} (x);
\end{tikzpicture}

\vspace{0.3cm}
{\small Преобразование является поворотом на $120^\circ$}
\end{center}
:::
::::

---

# 9. Задача 8: Симметрия рекомендательной системы

## Условие задачи

В платформе зарегистрировано 16 пользователей и загружено 16 фильмов. Каждый пользователь поставил фильму либо **лайк (+1)**, либо **дизлайк (-1)**.

Аналитики заметили аномалию:
Для любых двух **разных пользователей** найдется *ровно 8 фильмов*, оценки которым у них не совпали (один лайк, один дизлайк).

\vspace{0.5cm}
**Докажите**, что для любых двух **разных фильмов** обязательно найдется ровно 8 пользователей, которые оценили эти фильмы по-разному. *(Сложность: Очень сложная).*

## Формализация через матрицу Грама

Составим матрицу рейтингов $A \in \{-1, 1\}^{16 \times 16}$. Строки — пользователи, столбцы — фильмы.

:::: {.columns}
::: {.column width="55%"}
Возьмём две разные строки (пользователей) $r_i$ и $r_k$. Чему равно их скалярное произведение?
$$ \langle r_i, r_k \rangle = \sum_{j=1}^{16} A_{ij} A_{kj} $$

- Если оценки совпали, произведение равно $+1$.
- Если не совпали, произведение равно $-1$.

По условию несовпадений ровно 8. Оставшиеся 8 — совпадения.
$$ \langle r_i, r_k \rangle = 8(1) + 8(-1) = \mathbf{0} $$
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[scale=0.6, thick]
  \foreach \i in {1,2,3} {
    \node[circle, draw, fill=blue!20] (U\i) at (0, -\i*1.5) {$U_\i$};
    \node[circle, draw, fill=green!20] (M\i) at (4, -\i*1.5) {$M_\i$};
  }
  \draw (U1) -- (M1) node[midway, above] {+1};
  \draw[dashed, red] (U1) -- (M2) node[midway, below] {-1};
  \draw (U2) -- (M2) node[midway, above] {+1};
  \draw[dashed, red] (U2) -- (M3) node[midway, below] {-1};
\end{tikzpicture}

\vspace{0.2cm}
{\small Строки матрицы ортогональны!}
\end{center}
:::
::::

## Матричная магия (Левая обратная = Правая)

Раз все попарные скалярные произведения разных строк равны $0$, а скалярное произведение строки на саму себя равно $16$ (сумма 16 единиц), то матрица Грама:
$$ AA^T = 16 I $$

:::: {.columns}
::: {.column width="50%"}
Поделим обе части на 16:
$$ \left(\frac{1}{4}A\right) \left(\frac{1}{4}A^T\right) = I $$

Обозначим $B = \frac{1}{4}A$. Мы получили $BB^T = I$.
Как мы помним из теории, для квадратных матриц из $BB^T = I$ автоматически следует $B^T B = I$.
:::
::: {.column width="50%"}
Возвращаемся к $A$:
$$ A^T A = 16 I $$

Это означает, что скалярное произведение любых двух разных **столбцов** (фильмов) тоже равно нулю!

\vspace{0.3cm}
\begin{block}{Вывод}
Если сумма 16 произведений из $\pm 1$ равна нулю, то там ровно 8 плюсов и 8 минусов. Значит, несовпадений тоже ровно 8. Ч.т.д.
\end{block}
:::
::::

---

# 10. Задача 9: Инициализация Attention-масок

## Условие задачи

Генерируются две симметричные маски $A$ и $B$ **нечётного** размера $n \times n$.

- Ранг обеих матриц равен 1.
- Элементы принимают значения только $+1$ или $-1$.
- Матрицы независимы и равновероятны.

**Вопрос:** Найдите вероятность того, что они коммутируют ($AB = BA$). *(Сложность: Очень сложная).*

## Параметризация матриц ранга 1

Любая симметричная матрица из $\pm 1$ ранга 1 представима как $A = s_A u u^T$, где $u \in \{-1, 1\}^n$, а знак $s_A \in \{-1, 1\}$.

:::: {.columns}
::: {.column width="55%"}
Пусть $A = s_A u u^T$ и $B = s_B v v^T$.
Запишем условие $AB = BA$:
$$ (s_A u u^T)(s_B v v^T) = (s_B v v^T)(s_A u u^T) $$

Знаки сокращаются:
$$ u (u^T v) v^T = v (v^T u) u^T $$

Заметим, что $u^T v = v^T u = k$ (это просто скаляр, число).
$$ k \cdot (u v^T) = k \cdot (v u^T) $$
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[thick]
  \node at (0, 1.5) {$k = u^T v = \sum_{i=1}^n u_i v_i$};
  \node[font=\bfseries, red] at (0, 0) {$n$ — НЕЧЁТНОЕ!};
  \node at (0, -1) {\small Сумма нечётного числа};
  \node at (0, -1.5) {\small слагаемых $\pm 1 \neq 0$};
\end{tikzpicture}
\end{center}
:::
::::

## Чётность спасает от деления на ноль

Так как $n$ нечётно, сумма $n$ слагаемых, каждое из которых $\pm 1$, состоит из нечётного количества нечётных чисел.
**Она никогда не равна нулю!** ($k \neq 0$).

:::: {.columns}
::: {.column width="50%"}
Раз $k \neq 0$, смело делим на него:
$$ u v^T = v u^T $$

Матрицы слева и справа равны. Первый столбец слева — вектор $u$ (умноженный на константу). Справа — вектор $v$.
Значит, $u$ и $v$ коллинеарны.
Раз они состоят из $\pm 1$, то $u = \pm v$.
:::
::: {.column width="50%"}
Но $u u^T = (-v)(-v)^T = v v^T$.
Это означает, что сами матрицы Грама совпадают.
Коммутация возможна только если:
$B = A$ или $B = -A$.

\vspace{0.3cm}
\begin{block}{Ответ}
Для матрицы $A$ (из $2^n$ возможных) подходит ровно 2 варианта матрицы $B$.
Вероятность: $\frac{2}{2^n} = \mathbf{\frac{1}{2^{n-1}}}$.
\end{block}
:::
::::

---

# 11. Задача 10: Циклы в графе связей (Гроб)

## Условие задачи

Архитектура связей нейросети задана целочисленной квадратной матрицей $A \in \mathrm{Mat}_n(\mathbb{Z})$.
Пусть $p$ — простое число.

\vspace{0.5cm}
**Докажите**, что сумма элементов на главной диагонали матрицы после $p$ проходов сигнала сравнима по модулю $p$ с исходной суммой:
$$ \operatorname{tr}(A^p) \equiv \operatorname{tr}(A) \pmod p $$

*(Подсказка: считайте, что все собственные числа $\lambda_i$ матрицы $A$ — целые).*

## Переход в спектральное пространство

Вместо работы с элементами матрицы $A$, перейдём к её собственным значениям $\lambda_i$.

:::: {.columns}
::: {.column width="55%"}
Мы знаем тождества для следа:
$$ \operatorname{tr}(A) = \sum_{i=1}^n \lambda_i $$
$$ \operatorname{tr}(A^p) = \sum_{i=1}^n \lambda_i^p $$

Возьмём след $\operatorname{tr}(A)$ и возведём его в степень $p$:
$$ (\lambda_1 + \dots + \lambda_n)^p $$
Раскроем скобки по полиномиальной формуле (обобщение бинома Ньютона).
:::
::: {.column width="45%"}
\begin{center}
\begin{tikzpicture}[thick, >=Stealth]
  \node[circle, draw, fill=blue!10] (1) at (0, 1.5) {1};
  \node[circle, draw, fill=blue!10] (2) at (2, 1.5) {2};
  \node[circle, draw, fill=blue!10] (3) at (1, 0) {3};
  \draw[->] (1) edge[bend left] (2);
  \draw[->] (2) edge[bend left] (3);
  \draw[->] (3) edge[bend left] (1);
  \draw[->] (1) edge[loop left] (1);
\end{tikzpicture}

\vspace{0.2cm}
{\small Диагональ $A^p$ = число путей из узла в себя за $p$ шагов}
\end{center}
:::
::::

## Малая теорема Ферма и Бином

Все перекрёстные коэффициенты в разложении $(\sum \lambda_i)^p$ имеют вид $\frac{p!}{k_1! \dots k_n!}$.
Поскольку $p$ — простое, а $k_i < p$, факториалы в знаменателе не могут сократить $p$ в числителе. Все эти смешанные коэффициенты делятся на $p$!

:::: {.columns}
::: {.column width="50%"}
Значит, по модулю $p$ выживают только чистые степени:
$$ (\sum \lambda_i)^p \equiv \sum \lambda_i^p \pmod p $$
То есть:
$$ (\operatorname{tr}(A))^p \equiv \operatorname{tr}(A^p) \pmod p $$
:::
::: {.column width="50%"}
С другой стороны, след $\operatorname{tr}(A)$ — это целое число. По **Малой теореме Ферма**, для любого целого $x$:
$$ x^p \equiv x \pmod p $$

Следовательно:
$$ (\operatorname{tr}(A))^p \equiv \operatorname{tr}(A) \pmod p $$

\begin{block}{Итог}
Совмещая два сравнения: \\
$\operatorname{tr}(A^p) \equiv \operatorname{tr}(A) \pmod p$.
\end{block}
:::
::::

---

# 12. Итоги занятия

## Чек-лист: Линейная алгебра в действии

:::: {.columns}
::: {.column width="60%"}

1. **Геометрия ML-моделей.**
   - Выход Softmax — это распределение (сумма 1).
   - Свойство $\sigma(-x) = 1 - \sigma(x)$ сводит задачу к центру масс датасета.
   - Оптимизация — это динамические системы (ищите собственные числа для анализа прыжков).
2. **Вынесение за скобки (GPU трюк).** Сводите суммы обратных матриц $A^{-1}+C^{-1}$ к инвертированию через тождество Вудбери.
3. **Следы и Многочлены.** След матрицы хранит весь её спектр. Комбинируя Малую теорему Ферма и бином на следах, можно доказывать свойства графов.
4. **Матрицы Грама и Симметрия.** Если нужно работать с попарными совпадениями, собирайте $A A^T$. Для квадратных матриц $AB=I \implies BA=I$.
:::
::: {.column width="40%"}
\vspace{1cm}
\begin{tikzpicture}
  \node[draw, fill=blue!10, text width=5cm, rounded corners, inner sep=10pt] {
    \textbf{Главная мысль:}\\
    Формулы в ML — это не просто вычисления. Переходите в спектральное или геометрическое пространство, и ответ найдётся в 2 строчки!
  };
\end{tikzpicture}
:::
::::
